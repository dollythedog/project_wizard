"""
Draft Agent for AI-powered document generation.

Enriches user inputs with AI, then renders templates.
"""

import json
from typing import Optional
from dataclasses import dataclass
from pathlib import Path
from jinja2 import Environment, FileSystemLoader, Template
from datetime import datetime

from app.services.blueprint_registry import get_registry
from app.services.ai_agents.llm_client import LLMClient
from app.services.ai_agents.context_builder import ProjectContext
from app.services.ai_agents.step_back_agent import StepBackResult
from app.models.blueprint import GenerationStrategy


@dataclass
class DraftResult:
    """Result from draft generation."""
    content: str
    model_used: str
    tokens_used: int
    sections_generated: list[str]


class DraftAgent:
    """
    Agent that generates document drafts.
    
    Uses blueprint templates, prompts, and project context to generate
    high-quality document sections.
    """
    
    def __init__(self, llm_client: LLMClient):
        """
        Initialize draft agent.
        
        Args:
            llm_client: LLM client for API calls
        """
        self.llm_client = llm_client
        self.blueprint_registry = get_registry()
    
    def generate_draft(
        self,
        template_name: str,
        user_inputs: dict[str, any],
        project_context: Optional[ProjectContext] = None,
        step_back_result: Optional[StepBackResult] = None
    ) -> DraftResult:
        """
        Generate a complete document draft.
        
        Process branches based on blueprint's generation_strategy:
        
        FIELD_ENRICHMENT:
        1. Use AI to ENRICH user inputs (expand brief answers into detailed content)
        2. Render template.j2 with enriched values
        3. Return formatted markdown
        
        SKELETON_OF_THOUGHT:
        1. Generate skeleton outline with key points per section
        2. Expand each section in detail using the skeleton
        3. Assemble sections into final document
        4. Return formatted markdown
        
        Args:
            template_name: Name of document template
            user_inputs: User-provided inputs for template
            project_context: Optional project context
            step_back_result: Optional step-back analysis result
            
        Returns:
            DraftResult with rendered template content
        """
        # Load blueprint and prompts
        blueprint = self.blueprint_registry.load_blueprint(template_name)
        prompts = self.blueprint_registry.load_prompts(template_name)
        draft_config = prompts.get("draft_generation", {})
        
        # Branch based on generation strategy
        if blueprint.generation_strategy == GenerationStrategy.SKELETON_OF_THOUGHT:
            return self._generate_with_skeleton(
                blueprint,
                prompts,
                user_inputs,
                project_context,
                step_back_result
            )
        else:
            # Default: field enrichment
            return self._generate_with_field_enrichment(
                blueprint,
                draft_config,
                template_name,
                user_inputs,
                project_context,
                step_back_result
            )
    
    def _build_system_message(self, draft_config: dict) -> str:
        """Build system message for LLM."""
        parts = []
        
        # Identity
        if "identity" in draft_config:
            parts.append(draft_config["identity"])
        
        # Goals
        if "goals" in draft_config:
            parts.append("\nYour goals:")
            for goal in draft_config["goals"]:
                parts.append(f"- {goal}")
        
        # Quality standards
        if "quality_standards" in draft_config:
            parts.append("\nQuality Standards:")
            for standard, description in draft_config["quality_standards"].items():
                parts.append(f"**{standard.replace('_', ' ').title()}:** {description}")
        
        # Rules
        if "rules" in draft_config:
            rules = draft_config["rules"]
            if "do" in rules:
                parts.append("\nDO:")
                for rule in rules["do"]:
                    parts.append(f"- {rule}")
            if "do_not" in rules:
                parts.append("\nDO NOT:")
                for rule in rules["do_not"]:
                    parts.append(f"- {rule}")
        
        # Tone and style
        if "tone_and_style" in draft_config:
            tone = draft_config["tone_and_style"]
            parts.append("\nTone and Style:")
            for key, value in tone.items():
                parts.append(f"- {key.replace('_', ' ').title()}: {value}")
        
        return "\n".join(parts)
    
    def _enrich_inputs(
        self,
        user_inputs: dict,
        project_context: Optional[ProjectContext],
        step_back_result: Optional[StepBackResult],
        draft_config: dict
    ) -> dict:
        """
        Use AI to enrich user inputs with detailed, professional content.
        
        Takes brief user inputs and expands them into 2-3 paragraph detailed content.
        Preserves arrays (from multiselect/checkbox inputs) as-is.
        """
        system_message = self._build_system_message(draft_config)
        
        prompt_parts = [
            "# TASK: ENRICH USER INPUTS",
            "",
            "You will receive brief user inputs for a project charter.",
            "Your job is to EXPAND each input into detailed, professional content (2-3 paragraphs each).",
            "Use the project context and step-back analysis to add relevant details.",
            ""
        ]
        
        # Add project context
        if project_context:
            prompt_parts.extend([
                "## PROJECT CONTEXT (use this to enrich inputs)",
                project_context.full_context_text,
                ""
            ])
        
        # Add step-back analysis
        if step_back_result:
            prompt_parts.extend([
                "## STEP-BACK ANALYSIS (use this to enrich inputs)",
                step_back_result.summary,
                ""
            ])
        
        # Separate inputs: strings to enrich vs. arrays/numbers to preserve
        inputs_to_enrich = {}
        inputs_to_preserve = {}
        
        for key, value in user_inputs.items():
            # Skip empty or 'done' values
            if not value or value == 'done':
                continue
            # Preserve lists (from multiselect checkboxes), numbers, bools as-is
            # These come from form controls, not user-written text
            if isinstance(value, (list, int, float, bool)):
                inputs_to_preserve[key] = value
            else:
                # Text and textarea fields get enriched
                inputs_to_enrich[key] = value
        
        # Add inputs to enrich to prompt
        prompt_parts.extend([
            "## USER INPUTS TO ENRICH",
            ""
        ])
        
        for key, value in inputs_to_enrich.items():
            prompt_parts.append(f"### {key}")
            prompt_parts.append(f"Original: {value}")
            prompt_parts.append("")
        
        prompt_parts.extend([
            "## INSTRUCTIONS",
            "",
            "For EACH input above, expand it into detailed, professional content:",
            "- Add 2-3 paragraphs of detail per input",
            "- If the input contains a markdown list (bullets with - or *), expand EACH bullet point",
            "- **CRITICAL:** Use SPECIFIC numbers, metrics, and data points from the Project Context notes",
            "- Examples: dollar amounts, headcount, percentages, timelines, rates",
            "- DO NOT use vague terms like 'significant' or 'substantial' when specific numbers exist in notes",
            "- Reference specific roles, departments, and stakeholders mentioned in the context",
            "- Maintain professional PM tone while being concrete and data-driven",
            "",
            "Return ONLY a JSON object with enriched values:",
            '{"input_name": "enriched content here (2-3 paragraphs)", ...}',
            "",
            "IMPORTANT: Return ONLY valid JSON, no other text."
        ])
        
        prompt = "\n".join(prompt_parts)
        
        # Generate enriched inputs
        response = self.llm_client.generate(
            prompt=prompt,
            system_message=system_message,
            temperature=0.7,
            max_tokens=6000  # Increased for detailed content
        )
        
        try:
            # Claude sometimes wraps JSON in markdown code blocks
            content = response.content.strip()
            
            # Remove markdown code fences
            if content.startswith("```"):
                # Find the actual JSON content
                lines = content.split("\n")
                # Remove first line (```json or ```) and find closing ```
                json_lines = []
                in_json = False
                for line in lines:
                    if line.strip().startswith("```") and not in_json:
                        in_json = True
                        continue
                    elif line.strip() == "```" and in_json:
                        break
                    elif in_json:
                        json_lines.append(line)
                content = "\n".join(json_lines).strip()
            
            # Try to parse JSON
            enriched = json.loads(content)
            
            # Validate it's a dict
            if not isinstance(enriched, dict):
                raise ValueError("Response is not a JSON object")
            
            # Add back preserved inputs (arrays, numbers, dates)
            enriched.update(inputs_to_preserve)
            # Add back any inputs that weren't enriched
            for key, value in user_inputs.items():
                if key not in enriched:
                    enriched[key] = value
            # Track tokens
            enriched['_tokens_used'] = response.tokens_used
            return enriched
            
        except (json.JSONDecodeError, ValueError) as e:
            # Fallback: return original inputs
            print(f"Failed to parse enriched inputs: {e}")
            print(f"Response length: {len(response.content)}")
            print(f"Response start: {response.content[:200]}")
            print(f"Response end: {response.content[-200:]}")
            # Return originals with token tracking
            result = user_inputs.copy()
            result['_tokens_used'] = response.tokens_used
            return result
    
    def _render_template(
        self,
        template_name: str,
        enriched_inputs: dict
    ) -> str:
        """
        Render Jinja2 template with enriched input values.
        """
        # Get template path
        template_path = self.blueprint_registry.get_template_path(template_name)
        
        # Set up Jinja2 environment
        template_dir = template_path.parent
        env = Environment(loader=FileSystemLoader(str(template_dir)))
        
        # Load template
        template = env.get_template(template_path.name)
        
        # Convert date strings to datetime objects if needed
        context = enriched_inputs.copy()
        if 'charter_date' in context and isinstance(context['charter_date'], str):
            try:
                context['charter_date'] = datetime.strptime(context['charter_date'], '%Y-%m-%d')
            except:
                context['charter_date'] = datetime.now()
        
        # Render template
        rendered = template.render(**context)
        return rendered
